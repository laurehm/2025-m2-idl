{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea236a4",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning: Cours-1\n",
    "_Adapted from [Dataflowr Module 1](https://dataflowr.github.io/website/modules/1-intro-general-overview/) by Marc Lelarge_\n",
    "\n",
    "Let's start with an existing model for one of the most popular task in machine learning: image classification.\n",
    "In this notebook, we focus on the [Dogs vs Cats competition](https://www.kaggle.com/c/dogs-vs-cats) at Kaggle.\n",
    "\n",
    "## System setup\n",
    "\n",
    "Import the required packages, check the current version of PyTorch, and check that GPU is available (on Colab you may need to change the runtime first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c603376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import time\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"Using {device=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a359a38",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "There are 25,000 labeled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle website, when this competition was launched (end of 2013): \"State of the art: The current literature suggests machine classifiers can score above 80% accuracy on this task\". So if you can beat 80%, then you will be at the cutting edge as of 2013!\n",
    "\n",
    "Jeremy Howard (fast.ai) provides a direct link to the dogscats dataset. He's separated the cats and dogs into separate folders and created a validation folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uu00t437q6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir data\n",
    "# the following line should be modified if you run the notebook on your computer\n",
    "# change directory to data where you will store the dataset\n",
    "%cd data/ #%cd /content/data/\n",
    "!wget http://files.fast.ai/data/examples/dogscats.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca850ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxvf dogscats.tgz\n",
    "%cd dogscats/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931326c",
   "metadata": {},
   "source": [
    "Here is the file tree of `dogscats`:\n",
    "```bash\n",
    ".\n",
    "├── test1 # contains 12500 images of cats and dogs\n",
    "├── train\n",
    "|   └── cats # contains 11500 images of cats\n",
    "|   └── dogs # contains 11500 images of dogs\n",
    "├── valid\n",
    "|   └── cats # contains 1000 images of cats\n",
    "|   └── dogs # contains 1000 images of dogs\n",
    "├── sample\n",
    "|   └── train\n",
    "|       └── cats # contains 8 images of cats\n",
    "|       └── dogs # contains 8 images of dogs    \n",
    "|   └── valid \n",
    "|       └── cats # contains 4 images of cats\n",
    "|       └── dogs # contains 4 images of dogs    \n",
    "├── models # empty folder\n",
    "```\n",
    "\n",
    "12,500 images are in the `test1` sub-folder; the dataset of 25,000 labeled images has been split into a training set and a validation set.\n",
    "\n",
    "The sub-folder `sample` is here only to make sure the code is running properly on a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38102b24",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440195d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dogscats' # modify if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9294a9a",
   "metadata": {},
   "source": [
    "`datasets` is a class defined in the `torchvision` package (see [torchvision.datasets](http://pytorch.org/docs/master/torchvision/datasets.html)) for data loading. It integrates a multi-threaded loader that fetches images from the disk, groups them in mini-batches and serves them continuously to the GPU right after each _forward_/_backward_ pass through the network.\n",
    "\n",
    "Images need a bit of preparation before passing them through the network. They need to all have the same size $224\\times 224 \\times 3$ plus some extra formatting done below by the normalize transform (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a64189",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "imagenet_format = transforms.Compose([\n",
    "                transforms.CenterCrop(224), # Crop the image to 224 px\n",
    "                transforms.ToTensor(), # Convert to Torch tensors\n",
    "                normalize, # Normalize\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), imagenet_format)\n",
    "         for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34399",
   "metadata": {},
   "source": [
    "We used `datasets.ImageFolder` to load the datasets. Let's look at this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2841db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "?datasets.ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e488b",
   "metadata": {},
   "source": [
    "We see that `datasets.ImageFolder` has attributes: classes, class_to_idx, imgs.\n",
    "\n",
    "Let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcdf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_classes = dsets['train'].classes\n",
    "dset_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47469930",
   "metadata": {},
   "source": [
    "The names of the classes are directly inferred from the structure of the folder:\n",
    "```bash\n",
    "├── train\n",
    "|   └── cats\n",
    "|   └── dogs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c110d6",
   "metadata": {},
   "source": [
    "Label 0 corresponds to cats and 1 to dogs.\n",
    "\n",
    "We can see that the first 5 elements of the train dataset are pairs (location_of_the_image, label): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets['train'].imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee30749",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'valid']}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf13296",
   "metadata": {},
   "source": [
    "As expected, we have 23,000 images in the training set and 2,000 in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbafa26",
   "metadata": {},
   "source": [
    "The `torchvision` packages allows complex pre-processing/transforms of the input data (_e.g._ normalization, cropping, flipping, jittering). A sequence of transforms can be grouped in a pipeline with the help of the `torchvision.transforms.Compose` function, see [torchvision.transforms](http://pytorch.org/docs/master/torchvision/transforms.html).\n",
    "\n",
    "The magic help `?` allows you to retrieve function you defined and forgot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "?imagenet_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83410d4c",
   "metadata": {},
   "source": [
    "Where do this normalization and the magic constants for `mean` and `std` come from?\n",
    "\n",
    "As explained in the [PyTorch doc](https://pytorch.org/docs/stable/torchvision/models.html), you will use a pretrained model. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`.\n",
    "\n",
    "We can now define 2 dataloaders for the train and valid datasets using the `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f508ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(dsets['train'], batch_size=64, shuffle=True, num_workers=6)\n",
    "loader_valid = torch.utils.data.DataLoader(dsets['valid'], batch_size=5, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(loader_valid)\n",
    "inputs_try, labels_try = next(iter(loader_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9010310",
   "metadata": {},
   "source": [
    "The validation dataset contains 2,000 images, hence there are 400 batches of size 5. `labels_try` contains the labels of the first batch and `inputs_try` contains the images of the first batch.\n",
    "But what is an image here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_try[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea478b",
   "metadata": {},
   "source": [
    "A 3-channel RGB image has shape (3 x H x W). Note that entries can be negative because of the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bc046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "#   Imshow for Tensor.\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = np.clip(std * inp + mean, 0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a grid from batch from the validation data\n",
    "out = torchvision.utils.make_grid(inputs_try)\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in labels_try])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc040ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(loader_train))\n",
    "\n",
    "n_images = 8\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697141f0",
   "metadata": {},
   "source": [
    "## The VGG model\n",
    "\n",
    "The torchvision module comes with a zoo of popular CNN architectures that are already trained on [ImageNet](http://www.image-net.org/) (1.2M training images). When called for the first time, if `pretrained=True` the model is fetched over the internet and downloaded to `~/.torch/models`.\n",
    "For subsequent calls, the model will be directly read from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23395012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = models.vgg16(weights='DEFAULT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef2d2d",
   "metadata": {},
   "source": [
    "We will first use the VGG model without any modification. In order to interpret the results, we need to import the 1,000 ImageNet categories, available at: [https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json](https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec23c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fpath = 'imagenet_class_index.json'\n",
    "\n",
    "with open(fpath) as f:\n",
    "    class_dict = json.load(f)\n",
    "dic_imagenet = [class_dict[str(i)][1] for i in range(len(class_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4132690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_imagenet[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62900294",
   "metadata": {},
   "source": [
    "Now let's try to run the model on our small input, and see the results.\n",
    "\n",
    "Note: In PyTorch, we need to transfer the input tensors to the device to use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28265ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_try , labels_try = inputs_try.to(device), labels_try.to(device)\n",
    "\n",
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try = model_vgg(inputs_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_try.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577eceb2",
   "metadata": {},
   "source": [
    "To translate the outputs of the network into 'probabilities', we pass it through a [Softmax function](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_softm = nn.Softmax(dim=1)\n",
    "probs = m_softm(outputs_try)\n",
    "vals_try,preds_try = torch.max(probs,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d0d26",
   "metadata": {},
   "source": [
    "Let's check that we obtain probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(probs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6447d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4637f4c",
   "metadata": {},
   "source": [
    "The predictions correspond to the labels of the ImageNet categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c53a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_try)\n",
    "print([dic_imagenet[i] for i in preds_try.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff04bb",
   "metadata": {},
   "source": [
    "Here are the predictions (label with maximum probability) and the corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(inputs_try.data.cpu())\n",
    "\n",
    "imshow(out, title=[dic_imagenet[i] for i in preds_try.data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe7c00",
   "metadata": {},
   "source": [
    "## Modify the last layer and freeze the rest\n",
    "\n",
    "Let's look at the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271378a",
   "metadata": {},
   "source": [
    "We'll learn about what these different blocks do later in the course. For now, it's enough to know that:\n",
    "\n",
    "- Convolution layers are for finding small to medium size patterns in images -- analyzing the images locally\n",
    "- Dense (fully connected) layers are for combining patterns across an image -- analyzing the images globally\n",
    "- Pooling layers downsample -- in order to reduce image size and to improve invariance of learned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496adea9",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "![vgg16](https://dataflowr.github.io/notebooks/Module1/img/vgg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1d5ca",
   "metadata": {},
   "source": [
    "In this practical example, our goal is to use the already trained model and just change the number of output classes. To this end, we replace the last `nn.Linear` layer trained for 1,000 classes with one with 2 classes. In order to freeze the weights of the other layers during training, we set the field `requires_grad=False`. In this manner, no gradient will be computed for them during backprop and hence no update to the weights. Only the weights for the 2-class layer will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "model_vgg.classifier._modules['6'] = nn.Linear(4096, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcf9a7",
   "metadata": {},
   "source": [
    "PyTorch documentation for [LogSoftmax](https://pytorch.org/docs/stable/nn.html#logsoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_vgg.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = model_vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faae3ae",
   "metadata": {},
   "source": [
    "## Training the new Layer\n",
    "\n",
    "### Creating loss function and optimizer\n",
    "\n",
    "We choose a loss function for our classification task.\n",
    "The loss is the objective function we are trying to minimize during training.\n",
    "PyTorch documentation for [CrossEntropyLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.loss.CrossEntropyLoss.html) and the [torch.optim module](https://docs.pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer_vgg = torch.optim.SGD(model_vgg.classifier[6].parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720de89",
   "metadata": {},
   "source": [
    "We can now train our model to minimize the loss.\n",
    "This is a classic training loop:\n",
    "For each batch:\n",
    "- Execute the forward pass to compute the output of the network\n",
    "- Compute the loss using the output and the expected value\n",
    "- Execute the backward pass to compute the gradients\n",
    "- Update the parameters\n",
    "\n",
    "Repeat the entire process for several epochs (passes over the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dfc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,dataloader,size,epochs=1,optimizer=None):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs,classes in dataloader:\n",
    "            inputs, classes = inputs.to(device), classes.to(device) # move to GPU\n",
    "            outputs = model(inputs) # forward pass\n",
    "            loss = criterion(outputs,classes) # loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # backward pass\n",
    "            optimizer.step() # update\n",
    "            _,preds = torch.max(outputs.data,1)\n",
    "            # statistics\n",
    "            running_loss += loss.data.item()\n",
    "            running_corrects += torch.sum(preds == classes.data)\n",
    "        epoch_loss = running_loss / size\n",
    "        epoch_acc = running_corrects.data.item() / size\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model_vgg,loader_train,size=dset_sizes['train'],epochs=2,optimizer=optimizer_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a95f05",
   "metadata": {},
   "source": [
    "It is now time to test our new model. The following test function iterates over a dataset batch by batch, but we do not execute a backward pass or update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,dataloader,size):\n",
    "    model.eval()\n",
    "    predictions = np.zeros(size)\n",
    "    all_classes = np.zeros(size)\n",
    "    all_proba = np.zeros((size,2))\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs,classes in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,classes)           \n",
    "        _,preds = torch.max(outputs.data,1)\n",
    "            # statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == classes.data)\n",
    "        predictions[i:i+len(classes)] = preds.to('cpu').numpy()\n",
    "        all_classes[i:i+len(classes)] = classes.to('cpu').numpy()\n",
    "        all_proba[i:i+len(classes),:] = m_softm(outputs.data).to('cpu').numpy()\n",
    "        i += len(classes)\n",
    "    epoch_loss = running_loss / size\n",
    "    epoch_acc = running_corrects.data.item() / size\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch_loss, epoch_acc))\n",
    "    return predictions, all_proba, all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions, all_proba, all_classes = test_model(model_vgg,loader_valid,size=dset_sizes['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78e7f0",
   "metadata": {},
   "source": [
    "Let's visualize a few results in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(loader_valid))\n",
    "\n",
    "n_images = 7\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs[0:n_images])\n",
    "\n",
    "imshow(out, title=[dset_classes[x] for x in classes[0:n_images]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a6b80",
   "metadata": {},
   "source": [
    "As with the original VGG model, we can convert the output of the network into probabilities using a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_vgg(inputs[:n_images].to(device))\n",
    "print(m_softm(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07944a",
   "metadata": {},
   "source": [
    "## Model predictions\n",
    "\n",
    "The most important metrics for us to look at are for the validation set, since we want to check for over-fitting.\n",
    "\n",
    "With our first model, we should try to overfit before we start worrying about how to handle that - there's no point even thinking about regularization, data augmentation, etc. if you're still underfitting!\n",
    "\n",
    "As well as looking at the overall metrics, it's also a good idea to look at examples of each of:\n",
    "\n",
    "   1. A few correct labels at random\n",
    "   2. A few incorrect labels at random\n",
    "   3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "   4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "   5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "\n",
    "In general, these are particularly useful for debugging problems in the model. Since our model is very simple, there may not be too much to learn at this stage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca66d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images to view for each visualization task\n",
    "n_view = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oab84ejjgxo",
   "metadata": {},
   "source": [
    "### 1. Correct predictions\n",
    "\n",
    "First, let's compute the overall accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.where(predictions==all_classes)[0]\n",
    "len(correct)/dset_sizes['valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nlq4dxy58dr",
   "metadata": {},
   "source": [
    "Our model achieves 97.9% accuracy! Let's look at a few random correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20618058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import permutation\n",
    "from IPython.display import Image, display\n",
    "for x in permutation(correct)[:n_view]:\n",
    "    display(Image(filename=dsets['valid'].imgs[x][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpm824b9vu",
   "metadata": {},
   "source": [
    "### 2. Incorrect predictions\n",
    "\n",
    "Now let's examine some incorrect predictions to understand where the model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(predictions!=all_classes)[0]\n",
    "for x in permutation(incorrect)[:n_view]:\n",
    "    print(dsets['valid'].imgs[x][1], predictions[x])\n",
    "    display(Image(filename=dsets['valid'].imgs[x][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8gkrazvedc",
   "metadata": {},
   "source": [
    "### 3. Most confident correct predictions\n",
    "\n",
    "We now look at the predictions where the model was most confident and correct. For cats, we sort by probability of class 0 (ascending order means lowest probability = highest confidence after we reverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_cats = np.where((predictions==0) & (predictions==all_classes))[0]\n",
    "most_correct_cats = np.argsort(all_proba[correct_cats,0])[:n_view]\n",
    "\n",
    "for x in most_correct_cats:\n",
    "    print(dsets['valid'].imgs[correct_cats[x]][1], predictions[correct_cats[x]])\n",
    "    display(Image(filename=dsets['valid'].imgs[correct_cats[x]][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892ye7t5l5",
   "metadata": {},
   "source": [
    "Similarly, we can look at the images the model was most confident were dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dogs = np.where((predictions==1) & (predictions==all_classes))[0]\n",
    "most_correct_dogs = np.argsort(all_proba[correct_dogs,1])[:n_view]\n",
    "\n",
    "for x in most_correct_dogs:\n",
    "    print(dsets['valid'].imgs[correct_dogs[x]][1], predictions[correct_dogs[x]])\n",
    "    display(Image(filename=dsets['valid'].imgs[correct_dogs[x]][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51tqppf1at8",
   "metadata": {},
   "source": [
    "### 4. Most confident incorrect predictions (most wrong)\n",
    "\n",
    "Now, these are the images where the model was very confident but wrong. We look for images predicted as cats (class 0) but which are actually dogs (class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2196a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_cats = np.where((predictions==0) & (predictions!=all_classes))[0]\n",
    "most_incorrect_cats = np.argsort(all_proba[incorrect_cats, 0])[:n_view]\n",
    "\n",
    "for x in most_incorrect_cats:\n",
    "    print(dsets['valid'].imgs[incorrect_cats[x]][1], predictions[incorrect_cats[x]])\n",
    "    display(Image(filename=dsets['valid'].imgs[incorrect_cats[x]][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rtl88l1unfg",
   "metadata": {},
   "source": [
    "### 5. Most uncertain predictions\n",
    "\n",
    "Finally, let's look at the predictions where the model was most uncertain. These are images where the probability is closest to 0.5 (the model cannot decide between cat and dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty = np.abs(all_proba[:, 1] - 0.5)\n",
    "most_uncertain = np.argsort(uncertainty)[:n_view]\n",
    "\n",
    "for x in most_uncertain:\n",
    "    print(dsets['valid'].imgs[x][1], all_proba[x, :])\n",
    "    display(Image(filename=dsets['valid'].imgs[x][0], retina=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5538bdb",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "What did we do in the end? We probably killed a fly with a sledgehammer!\n",
    "\n",
    "In our case, the sledgehammer is VGG pretrained on ImageNet, a dataset containing a lot of pictures of cats and dogs. Indeed, we saw that without modification the network was able to predict dog and cat breeds. Hence, it is not very surprising that the features computed by VGG are very accurate for our classification task. In the end, we need to learn only the parameters of the last linear layer, i.e., 8,194 parameters (do not forget the bias $2\\times 4096+2$). Indeed, this can be done on CPU without any problem.\n",
    "\n",
    "Nevertheless, this example is still instructive as it shows all the necessary steps in a deep learning project. Here we did not struggle with the learning process of a deep network, but we did all the preliminary engineering tasks:\n",
    " \n",
    "- downloading a dataset, \n",
    "- setting up the environment to use a GPU, \n",
    "- preparing the data, \n",
    "- use a pretrained model\n",
    "- retrain with a new layer on a different task...\n",
    "\n",
    "These steps are essential in any deep learning project and a necessary requirement before having fun playing with network architectures and understanding the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eee39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2-idl (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
