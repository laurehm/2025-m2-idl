{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FM8CQHKcfhx"
   },
   "source": [
    "# TP: Simple implementation of backprop\n",
    "\n",
    "_From [Dataflowr Module 2](https://dataflowr.github.io/website/modules/2b-automatic-differentiation/) by Marc Lelarge_\n",
    "\n",
    "Here we implement a simple backpropagation algorithm with `numpy` for the following problem:\n",
    "\n",
    "We generate points $(x_t,y_t)$ where $y_t= \\exp(w^*x_t+b^*)$, i.e $y_t$ is obtained by applying a deterministic function to $x_t$ with parameters $w^*$ and $b^*$. Our goal is to recover the parameters $w^*$ and $b^*$ from the observations $(x_t,y_t)$.\n",
    "\n",
    "[Last week](2-basics/linear-regression.ipynb), we computed the loss on the entire dataset at each iteration of the learning loop using vectorized operations. This is called **Batched Gradient Descent**.\n",
    "\n",
    "At the other extreme, it is also possible to compute the gradient after a single data point to update the parameters. This is called **Stochastic Gradient Descent**.\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The total loss over the entire dataset is:\n",
    "$$\n",
    "L = \\sum_t\\underbrace{\\left(\\exp(w x_t+b)-y_t \\right)^2}_{L_t}.\n",
    "$$\n",
    "\n",
    "Instead of computing the gradient over the **entire dataset** $\\frac{\\partial L}{\\partial w} = \\sum_t \\frac{\\partial L_t}{\\partial w}$ at each iteration (which is expensive), we:\n",
    "\n",
    "1. **Randomly pick one sample** $t$ at each iteration\n",
    "2. Compute the gradient of **only that sample's loss**: $\\frac{\\partial L_t}{\\partial w}$ and $\\frac{\\partial L_t}{\\partial b}$\n",
    "3. Update the parameters using this **single-sample gradient**\n",
    "\n",
    "The SGD update loop is:\n",
    "\n",
    "For each iteration $i$:\n",
    "- Pick a random sample index $t \\in \\{1,\\dots,n\\}$\n",
    "- Update parameters:\n",
    "\n",
    "\\begin{align*}\n",
    "w_{i+1}&=w_{i}-\\alpha\\frac{\\partial{L_t}}{\\partial w} \\\\\n",
    "b_{i+1}&=b_{i}-\\alpha\\frac{\\partial{L_t}}{\\partial b}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha>0$ is the **learning rate**.\n",
    "\n",
    "Although each gradient $\\frac{\\partial L_t}{\\partial w}$ is a noisy estimate of the true gradient $\\frac{\\partial L}{\\partial w}$, on average it points in the right direction. Over many iterations, the parameters converge to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need PyTorch for these practicals\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQlF4jhfcfiD"
   },
   "outputs": [],
   "source": [
    "w, b = 0.5, 2\n",
    "xx = np.arange(0,1,.01)\n",
    "yy = np.exp(w*xx+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OIvcmsqcfiL"
   },
   "outputs": [],
   "source": [
    "plt.plot(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siSZvSBBcfiW"
   },
   "source": [
    "Following what we just saw in the course, you need to implement each of the basic operations: `(.*w), (.+b), exp(.)` with a forward method, a backward method and a step method (to update the parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xfy5oCZBMnVj"
   },
   "source": [
    "To help you, I have implemented the `(.+b)` operation as a Python class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeY2DlqNcfiZ"
   },
   "outputs": [],
   "source": [
    "class add_bias(object):\n",
    "    def __init__(self,b):\n",
    "        # initialize with a bias b\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # return the result of adding the bias      \n",
    "        return x + self.b\n",
    "    \n",
    "    def backward(self,grad):\n",
    "        # save the gradient (to update the bias in the step method) and return the gradient backward\n",
    "        self.grad = grad\n",
    "        return grad\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        # update the bias\n",
    "        self.b -= learning_rate*self.grad        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBLMKY1nMnVm"
   },
   "source": [
    "Consider now a simpler problem where you have $z_t = x_t+b^*$ and your task is to estimate $b^*$ by minimizing the loss $\\sum_t(x_t+b-z_t)^2$ as a function of $b$ with SGD. You can use the `add_bias` defined above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZpxeNqQMnVn"
   },
   "outputs": [],
   "source": [
    "# first compute the z_t with a true bias of 5:\n",
    "zz = xx+5\n",
    "\n",
    "#start with an initial guess of 1 for the bias:\n",
    "My_add_bias = add_bias(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLoIijKPMnVq"
   },
   "outputs": [],
   "source": [
    "j = 10\n",
    "# your prediction will be for each sample\n",
    "z_pred = My_add_bias.forward(xx[j])\n",
    "z_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ppGMgc7MnVt"
   },
   "outputs": [],
   "source": [
    "# start with the gradient of the quadratic loss\n",
    "grad = 2*(z_pred-zz[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVIy0xeAMnVv"
   },
   "outputs": [],
   "source": [
    "# backpropagate the gradient to the parameter b\n",
    "My_add_bias.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJtZICCiMnVy"
   },
   "outputs": [],
   "source": [
    "# make an update of the bias\n",
    "My_add_bias.step(1e-2)\n",
    "My_add_bias.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZWF5vaTMnV0"
   },
   "source": [
    "The code above corresponds to one SGD update.\n",
    "Below, I coded the training loop for SGD where the update on the parameter is done each time you see a sample: for each sample $j$, you compute the associated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_p1nC0dpMnV1"
   },
   "outputs": [],
   "source": [
    "My_add_bias = add_bias(1)\n",
    "estimated_b = [1]\n",
    "for i in range(500):\n",
    "    # take a random indice\n",
    "    j = np.random.randint(1, len(xx))\n",
    "    z_pred = My_add_bias.forward(xx[j])\n",
    "    grad = 2*(z_pred-zz[j])\n",
    "    _ = My_add_bias.backward(grad)\n",
    "    My_add_bias.step(1e-2)\n",
    "    estimated_b.append(My_add_bias.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3cekk-nMnV4"
   },
   "outputs": [],
   "source": [
    "plt.plot(estimated_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaqpsHK6MnV6"
   },
   "source": [
    "Although SGD is computing a noisy version of the gradient, we see that SGD converges to the right solution in this case.\n",
    "\n",
    "Now it is your turn!\n",
    "Following what we just saw in the course, you need to implement each of the basic operations: `(.*w), exp(.)` with a forward method, a backward method and a step method.\n",
    "\n",
    "**Remember**:\n",
    "\n",
    "- _Forward pass_ (compute and store): $x$, $u = wx$, $v = u+b$, $\\hat{y} = \\exp(v)$, $\\ell = (\\hat{y}-y)^2$\n",
    "\n",
    "- _Backward pass_ steps:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{1. Initialize:} & \\frac{\\partial \\ell}{\\partial \\ell} &= 1\\\\\\\\\n",
    "\\\\\\\\\n",
    "&\\text{2. Loss:} & \\frac{\\partial \\ell}{\\partial \\hat{y}} &= \\frac{\\partial \\ell}{\\partial \\ell} \\cdot \\frac{\\partial \\ell}{\\partial \\hat{y}} = 1 \\cdot 2(\\hat{y}-y) = 2(\\hat{y}-y)\\\\\\\\\n",
    "\\\\\\\\\n",
    "&\\text{3. Exponential:} & \\frac{\\partial \\ell}{\\partial v} &= \\frac{\\partial \\ell}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial v} = 2(\\hat{y}-y) \\cdot \\hat{y}\\\\\\\\\n",
    "\\\\\\\\\n",
    "&\\text{4. Addition:} & \\frac{\\partial \\ell}{\\partial u} &= \\frac{\\partial \\ell}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} = 2(\\hat{y}-y) \\cdot \\hat{y} \\cdot 1\\\\\\\\\n",
    "\\\\\\\\\n",
    "&\\text{5. Multiplication:} & \\frac{\\partial \\ell}{\\partial w} &= \\frac{\\partial \\ell}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w} = 2(\\hat{y}-y) \\cdot \\hat{y} \\cdot x\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58L4gjz1MnV7"
   },
   "outputs": [],
   "source": [
    "class multiplication_weight(object):\n",
    "    def __init__(self, w):\n",
    "        # initialize with a weight w\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return the result of multiplying by weight\n",
    "        pass\n",
    "               \n",
    "    def backward(self,grad):\n",
    "        # save the gradient (to update the weight in the step method) and return the gradient backward\n",
    "        pass\n",
    "            \n",
    "    def step(self, learning_rate):\n",
    "        # update the weight\n",
    "        pass\n",
    "        \n",
    "class my_exp(object):\n",
    "    # no parameter\n",
    "    def forward(self, x):\n",
    "        # return exp(x)\n",
    "        pass\n",
    "\n",
    "    def backward(self,grad):\n",
    "        # return the gradient backward\n",
    "        pass\n",
    "            \n",
    "    def step(self, learning_rate):\n",
    "        # any parameter to update?\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koymOfVlcfih"
   },
   "source": [
    "Now, you will need to compose sequentially these operations and here you need to code a class composing operations. This class will have a forward, a backward and a step method and also a compute_loss method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EM5M8GCecfik"
   },
   "outputs": [],
   "source": [
    "class my_composition(object):\n",
    "    def __init__(self, layers):\n",
    "        # initialize with all the operations (called layers here!) in the right order...\n",
    "        pass\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # apply the forward method of each layer\n",
    "        pass\n",
    "            \n",
    "    def compute_loss(self, y_est, y_target):\n",
    "        # use the L2 loss\n",
    "        # return the loss and save the gradient of the loss\n",
    "        pass\n",
    "            \n",
    "    def backward(self):\n",
    "        # apply backprop sequentially, starting from the gradient of the loss\n",
    "        # Hint: https://docs.python.org/3/library/functions.html#reversed\n",
    "        pass\n",
    "            \n",
    "    def step(self, learning_rate):\n",
    "        # apply the step method of each layer\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hs1M-kebcfir"
   },
   "source": [
    "Now you need to code the 'training' loop. Keep track of the loss, weight and bias computed at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWJgoCEKcfiu"
   },
   "outputs": [],
   "source": [
    "my_fit = my_composition([multiplication_weight(1),add_bias(1), my_exp()])\n",
    "learning_rate = 1e-4\n",
    "losses =[]\n",
    "ws = []\n",
    "bs = []\n",
    "for i in range(5000):\n",
    "    # take a random indice\n",
    "    j = np.random.randint(1, len(xx))\n",
    "    # you can compare with\n",
    "    #j = i % len(xx)\n",
    "    # compute the estimated value of y from xx[j] with the current values of the parameters\n",
    "    \n",
    "    # compute the loss and save it\n",
    "    \n",
    "    # update the parameters \n",
    "    \n",
    "    #and save them\n",
    "    ws.append(my_fit.layers[0].w)\n",
    "    bs.append(my_fit.layers[1].b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7mp0eEEcfi2"
   },
   "outputs": [],
   "source": [
    "my_fit.layers[0].w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t54ayZNDcfi9"
   },
   "outputs": [],
   "source": [
    "my_fit.layers[1].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Gqz5CxzcfjG"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaHnaX8jcfjN"
   },
   "outputs": [],
   "source": [
    "plt.plot(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ch28XtPUcfjV"
   },
   "outputs": [],
   "source": [
    "plt.plot(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4Qb9d65NFpo"
   },
   "source": [
    "Now you understand how Pytorch deals with automatic differentiation!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "02_backprop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "2025-m2-idl (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
