{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent\n",
    "\n",
    "In this notebook, we'll implement linear regression from scratch using PyTorch, predicting monthly rent for Paris apartments based on surface area.\n",
    "\n",
    "We will:\n",
    "1. Create a synthetic dataset based on Paris rental prices\n",
    "2. Implement gradient descent **manually** (computing derivatives by hand)\n",
    "3. Visualize the optimization process on the loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate a Synthetic Dataset\n",
    "\n",
    "We'll create a synthetic dataset representing Paris apartment rentals:\n",
    "- **Surface**: apartment size in m² (20 to 100 m²)\n",
    "- **Rent**: monthly rent in euros\n",
    "\n",
    "Typical Paris rent is around 30-35€/m², so we'll use:\n",
    "$$\\text{rent} = 32 \\times \\text{surface} + 200 + \\text{noise}$$\n",
    "\n",
    "The intercept (200€) represents fixed costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters (what we want to recover)\n",
    "TRUE_W = 32.0  # Price per m² (slope)\n",
    "TRUE_B = 200.0  # Fixed cost (intercept)\n",
    "\n",
    "# Generate data: 50 apartments\n",
    "n_samples = 50\n",
    "surface = torch.rand(n_samples) * 80 + 20  # Surface: 20-100 m²\n",
    "noise = torch.randn(n_samples) * 100  # Add some noise\n",
    "rent = TRUE_W * surface + TRUE_B + noise\n",
    "\n",
    "print(f\"Dataset: {n_samples} Paris apartments\")\n",
    "print(f\"Surface range: {surface.min():.1f} - {surface.max():.1f} m²\")\n",
    "print(f\"Rent range: {rent.min():.0f} - {rent.max():.0f} €\")\n",
    "print(f\"\\nTrue parameters: w={TRUE_W}, b={TRUE_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(surface.numpy(), rent.numpy(), alpha=0.6, s=50)\n",
    "plt.xlabel('Surface (m²)', fontsize=12)\n",
    "plt.ylabel('Monthly Rent (€)', fontsize=12)\n",
    "plt.title('Paris Apartment Rentals: Rent vs Surface', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is now to recover these parameters (32, 200) by training a very simple regression model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression Model\n",
    "\n",
    "We start with one of the most basic prediction models: a linear regression.\n",
    "\n",
    "Our model is:\n",
    "$$\\hat{y} = w \\cdot x + b$$\n",
    "\n",
    "where:\n",
    "- $x$ = surface (input)\n",
    "- $\\hat{y}$ = predicted rent (output)\n",
    "- $w$ = weight (price per m²)\n",
    "- $b$ = bias (fixed cost)\n",
    "\n",
    "Our goal is to find the \"best\" straight line that fits this dataset, i.e., that is the closest to all the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plot_regression_interactive(w, b):\n",
    "    \"\"\"Interactive plot with w and b sliders.\"\"\"\n",
    "    \n",
    "    x_line = torch.linspace(surface.min(), surface.max(), 100)\n",
    "    y_line = w * x_line + b\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Data points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=surface.numpy(),\n",
    "        y=rent.numpy(),\n",
    "        mode='markers',\n",
    "        name='Data',\n",
    "        marker=dict(size=8, opacity=0.6, color='blue'),\n",
    "    ))\n",
    "    \n",
    "    # Regression line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_line.numpy(),\n",
    "        y=y_line.numpy(),\n",
    "        mode='lines',\n",
    "        name=f'Regression Line',\n",
    "        line=dict(color='red', width=3)\n",
    "    ))\n",
    "    \n",
    "    # True line\n",
    "    y_true = TRUE_W * x_line + TRUE_B\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_line.numpy(),\n",
    "        y=y_true.numpy(),\n",
    "        mode='lines',\n",
    "        name='True Line',\n",
    "        line=dict(color='green', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'y = {w:.2f}x + {b:.2f}',\n",
    "        xaxis_title='Surface (m²)',\n",
    "        yaxis_title='Monthly Rent (€)',\n",
    "        width=900,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    plot_regression_interactive,\n",
    "    w=FloatSlider(min=-50, max=50, step=1, value=-3, description='w:'),\n",
    "    b=FloatSlider(min=0, max=500, step=10, value=10, description='b:')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "This is a very classic problem.\n",
    "To find the best candidate, we will try to minimize the mean squared error on the dataset.\n",
    "\n",
    "$$L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"Make predictions: y_hat = w * x + b\"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error\"\"\"\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the Loss Surface\n",
    "\n",
    "Let's visualize the loss function as a surface over the parameter space $(w, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "w_range = torch.linspace(TRUE_W - 10, TRUE_W + 10, 50)\n",
    "b_range = torch.linspace(TRUE_B - 100, TRUE_B + 100, 50)\n",
    "W_grid, B_grid = torch.meshgrid(w_range, b_range, indexing='xy')\n",
    "\n",
    "# Compute loss on the dataset\n",
    "Loss_grid = torch.zeros_like(W_grid)\n",
    "for i in range(len(w_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        y_pred = predict(surface, W_grid[j, i].item(), B_grid[j, i].item())\n",
    "        Loss_grid[j, i] = mse_loss(rent, y_pred)\n",
    "\n",
    "print(f\"\\nLoss range: {Loss_grid.min():.2f} to {Loss_grid.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Surface(\n",
    "        x=w_range,\n",
    "        y=b_range,\n",
    "        z=Loss_grid,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    scene=dict(\n",
    "        xaxis_title='w (slope)',\n",
    "        yaxis_title='b (intercept)',\n",
    "        zaxis_title='Loss (MSE)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
    "    ),\n",
    "    title='Loss Surface (fully centered data: intercept ≈ 0)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent\n",
    "\n",
    "To find the optimal values for $w$ and $b$, we will implement **Gradient Descent**.\n",
    "\n",
    "Gradient: $\\nabla L = \\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}\\right)$\n",
    "\n",
    "Update rule:\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "$$b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "But first, we need to compute the gradient of our loss function: $L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w \\cdot x_i + b))^2$\n",
    "\n",
    "Let's use the chain rule:\n",
    "\n",
    "### Gradient with respect to $w$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w \\cdot x_i - b)^2 \\right]$$\n",
    "\n",
    "$$= \\frac{1}{n} \\sum_{i=1}^{n} 2 \\cdot (-x_i) \\cdot (y_i - w \\cdot x_i - b) $$\n",
    "\n",
    "$$= -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\cdot x_i$$\n",
    "\n",
    "### Gradient with respect to $b$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w \\cdot x_i - b)^2 \\right]$$\n",
    "\n",
    "$$= \\frac{1}{n} \\sum_{i=1}^{n} 2 \\cdot (-1) \\cdot (y_i - w \\cdot x_i - b) $$\n",
    "\n",
    "$$= -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute gradients manually using the formulas derived above.\n",
    "    \n",
    "    dL/dw = -2/n * sum((y_true - y_pred) * x)\n",
    "    dL/db = -2/n * sum(y_true - y_pred)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    error = y_true - y_pred\n",
    "    \n",
    "    grad_w = -2 * (error * x).sum() / n\n",
    "    grad_b = -2 * error.sum() / n\n",
    "    \n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything to train our model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters (random starting point)\n",
    "w = torch.tensor([25])  # Start around 25\n",
    "b = torch.tensor([100]) # Start around 100\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.00001\n",
    "n_iterations = 200\n",
    "\n",
    "# Store history for visualization\n",
    "history = {\n",
    "    'w': [],\n",
    "    'b': [],\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "# Gradient descent loop\n",
    "for iteration in range(n_iterations):\n",
    "    # Forward pass - use CENTERED data\n",
    "    y_pred = predict(surface, w, b)\n",
    "    loss = mse_loss(rent, y_pred)\n",
    "    \n",
    "    # Compute gradients manually - use CENTERED data\n",
    "    grad_w, grad_b = compute_gradients(surface, rent, y_pred)\n",
    "    \n",
    "    # Update parameters\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "    \n",
    "    # Store history\n",
    "    history['w'].append(w.item())\n",
    "    history['b'].append(b.item())\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (iteration + 1) % 20 == 0:\n",
    "        print(f\"Iter {iteration+1:3d}: Loss={loss.item():8.2f}, w={w.item():.2f}, b={b.item():.2f}\")\n",
    "\n",
    "print(f\"\\nFinal (centered space): w={w.item():.2f}, b={b.item():.2f}\")\n",
    "print(f\"True parameters: w={TRUE_W:.2f}, b={TRUE_B:.2f}\")\n",
    "print(f\"Error: Δw={abs(w.item()-TRUE_W):.2f}, Δb={abs(b.item()-TRUE_B):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss over Iterations', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_interactive(w.item(), b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Surface(\n",
    "        x=w_range,\n",
    "        y=b_range,\n",
    "        z=Loss_grid,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )\n",
    "])\n",
    "\n",
    "# Add history\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=history['w'],\n",
    "    y=history['b'],\n",
    "    z=history['loss'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='red', symbol='circle'),\n",
    "    name='Start'\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=700,\n",
    "    scene=dict(\n",
    "        xaxis_title='w (slope)',\n",
    "        yaxis_title='b (intercept)',\n",
    "        zaxis_title='Loss (MSE)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
    "    ),\n",
    "    title='Loss Surface with Gradient Descent Trajectory'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Standardize the Dataset to Balance Gradients\n",
    "\n",
    "The issue is that $w$ and $b$ operate on very different scales, causing unbalanced gradient magnitudes.\n",
    "\n",
    "**Why this happens:**\n",
    "- The gradient for $w$ includes multiplication by $x$ (surface values: 20-100)\n",
    "- The gradient for $b$ doesn't include $x$\n",
    "- This creates an elongated \"elliptical bowl\" in the loss surface\n",
    "- Different parameters need very different learning rates to converge efficiently\n",
    "\n",
    "**Solution: Standardize the dataset!**\n",
    "\n",
    "Standardization transforms both features and targets to have mean=0 and std=1:\n",
    "$$x_{\\text{std}} = \\frac{x - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "Benefits:\n",
    "- Both gradients have similar magnitudes\n",
    "- Can use a single, larger learning rate\n",
    "- Faster convergence\n",
    "- Gradient descent follows a more direct path to the minimum\n",
    "- The loss surface becomes more \"circular\" (better conditioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process: Standardize the dataset\n",
    "surface_std = (surface - surface.mean()) / surface.std()\n",
    "rent_std = (rent - rent.mean()) / rent.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid in standardized space\n",
    "w_range = torch.linspace(-0.5, 2, 50)  # Centered around 1 (correlation)\n",
    "b_range = torch.linspace(-1, 1, 50)    # Centered around 0\n",
    "W_grid, B_grid = torch.meshgrid(w_range, b_range, indexing='xy')\n",
    "\n",
    "# Compute loss on the standardized dataset\n",
    "Loss_grid = np.zeros_like(W_grid)\n",
    "for i in range(len(w_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        y_pred = predict(surface_std, W_grid[j, i].item(), B_grid[j, i].item())\n",
    "        Loss_grid[j, i] = mse_loss(rent_std, y_pred)\n",
    "\n",
    "print(f\"\\nLoss range: {Loss_grid.min():.2f} to {Loss_grid.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Surface(\n",
    "        x=w_range,\n",
    "        y=b_range,\n",
    "        z=Loss_grid,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    scene=dict(\n",
    "        xaxis_title='w (slope)',\n",
    "        yaxis_title='b (intercept)',\n",
    "        zaxis_title='Loss (MSE)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
    "    ),\n",
    "    title='Loss Surface (fully centered data: intercept ≈ 0)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train again on the standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([0.5])  # at 0.5 for standardized\n",
    "b = torch.tensor([0.5])\n",
    "\n",
    "learning_rate = 0.1  # Can use much larger now!\n",
    "n_iterations = 200\n",
    "\n",
    "history = {'w': [], 'b': [], 'loss': []}\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    y_pred = predict(surface_std, w, b)\n",
    "    loss = mse_loss(rent_std, y_pred)\n",
    "    \n",
    "    grad_w, grad_b = compute_gradients(surface_std, rent_std, y_pred)\n",
    "    \n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "\n",
    "    \n",
    "    history['w'].append(w.item())\n",
    "    history['b'].append(b.item())\n",
    "    history['loss'].append(loss.item())\n",
    "    \n",
    "    if (iteration) % 20 == 0:\n",
    "        print(f\"Iter {iteration:3d}: Loss={loss.item():8.6f}, w={w.item():.4f}, b={b.item():.4f}\")\n",
    "\n",
    "\n",
    "w_original = (w * rent.std() / surface.std())\n",
    "b_original = (b * rent.std() + rent.mean() - w_original * surface.mean())\n",
    "print(f\"Final (original):     w={w_original.item():.2f}, b={b_original.item():.2f}\")\n",
    "print(f\"True parameters:      w={TRUE_W:.2f}, b={TRUE_B:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['loss'], linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss over Iterations', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_interactive(w_original.item(), b_original.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Surface(\n",
    "        x=w_range,\n",
    "        y=b_range,\n",
    "        z=Loss_grid,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8,\n",
    "        name='Loss Surface'\n",
    "    )\n",
    "])\n",
    "\n",
    "# Add history\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=history['w'],\n",
    "    y=history['b'],\n",
    "    z=history['loss'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=8, color='red', symbol='circle'),\n",
    "    name='Start'\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=700,\n",
    "    scene=dict(\n",
    "        xaxis_title='w (slope)',\n",
    "        yaxis_title='b (intercept)',\n",
    "        zaxis_title='Loss (MSE)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
    "    ),\n",
    "    title='Loss Surface with Gradient Descent Trajectory'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 2D contour plot to see the true gradient direction\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Contour(\n",
    "    x=w_range,\n",
    "    y=b_range,\n",
    "    z=Loss_grid,\n",
    "    colorscale='Viridis',\n",
    "    contours=dict(showlabels=True),\n",
    "    name='Loss Contours'\n",
    "))\n",
    "\n",
    "# Add trajectory in 2D\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=history['w'],\n",
    "    y=history['b'],\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=4, color='red'),\n",
    "    line=dict(color='red', width=2),\n",
    "    name='GD Path'\n",
    "))\n",
    "\n",
    "# Add start/end\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=[history['w'][0]], y=[history['b'][0]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='Yellow', symbol='x'),\n",
    "    name='Start'\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=[history['w'][-1]], y=[history['b'][-1]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color='yellow', symbol='x'),\n",
    "    name='End'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='2D Contour: Gradient Descent Path',\n",
    "    xaxis_title='w',\n",
    "    yaxis_title='b',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Autodiff Magic: Let PyTorch Compute Gradients\n",
    "\n",
    "So far, we've manually computed gradients using calculus. Now let's see how PyTorch's **autodiff** system can do this automatically!\n",
    "\n",
    "When you set `requires_grad=True` on a tensor, PyTorch tracks all operations on it and builds a computational graph. You can then call `.backward()` to automatically compute gradients.\n",
    "\n",
    "### Comparison: Manual vs Autodiff\n",
    "\n",
    "**Manual approach** (what we did before):\n",
    "1. Derive gradients by hand using calculus\n",
    "2. Implement `compute_gradients()` function\n",
    "3. Update parameters manually\n",
    "\n",
    "**Autodiff approach** (PyTorch magic):\n",
    "1. Mark parameters with `requires_grad=True`\n",
    "2. Compute loss\n",
    "3. Call `loss.backward()` → gradients computed automatically!\n",
    "4. Access gradients via `.grad` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: compute gradients both ways at a random point\n",
    "w_test = torch.tensor([0.7], requires_grad=True)\n",
    "b_test = torch.tensor([0.3], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = predict(surface_std, w_test, b_test)\n",
    "loss = mse_loss(rent_std, y_pred)\n",
    "\n",
    "# Autograd gradients\n",
    "loss.backward()\n",
    "grad_w_auto = w_test.grad.item()\n",
    "grad_b_auto = b_test.grad.item()\n",
    "\n",
    "# Manual gradients\n",
    "grad_w_manual, grad_b_manual = compute_gradients(surface_std, rent_std, y_pred.detach())\n",
    "grad_w_manual = grad_w_manual.item()\n",
    "grad_b_manual = grad_b_manual.item()\n",
    "\n",
    "print(\"Gradient Comparison at w=0.7, b=0.3:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"∂L/∂w:  Autograd={grad_w_auto:.6f}  |  Manual={grad_w_manual:.6f}\")\n",
    "print(f\"∂L/∂b:  Autograd={grad_b_auto:.6f}  |  Manual={grad_b_manual:.6f}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Difference: Δw={abs(grad_w_auto - grad_w_manual):.10f}, Δb={abs(grad_b_auto - grad_b_manual):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retry our training loop using PyTorch autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters as tensors with gradient tracking\n",
    "w_auto = torch.tensor([0.5], requires_grad=True) # Track gradients for w\n",
    "b_auto = torch.tensor([0.5], requires_grad=True) # Track gradients for b\n",
    "\n",
    "print(f\"Initial parameters: w={w_auto.item():.2f}, b={b_auto.item():.2f}\")\n",
    "print(f\"w.requires_grad = {w_auto.requires_grad}\")\n",
    "print(f\"b.requires_grad = {b_auto.requires_grad}\")\n",
    "\n",
    "learning_rate = 0.1\n",
    "n_iterations = 200\n",
    "\n",
    "history_auto = {'w': [], 'b': [], 'loss': []}\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Forward pass\n",
    "    y_pred = predict(surface_std, w_auto, b_auto)\n",
    "    loss = mse_loss(rent_std, y_pred)\n",
    "    \n",
    "    # Backward pass - AUTOGRAD MAGIC!\n",
    "    loss.backward()  # Computes ∂L/∂w and ∂L/∂b automatically\n",
    "    \n",
    "    # Manual parameter update (no autograd for this part)\n",
    "    with torch.no_grad():  # Temporarily disable gradient tracking\n",
    "        w_auto -= learning_rate * w_auto.grad # Update w\n",
    "        b_auto -= learning_rate * b_auto.grad # Update b\n",
    "        \n",
    "    # IMPORTANT: Zero gradients after each step\n",
    "    w_auto.grad.zero_()\n",
    "    b_auto.grad.zero_()\n",
    "    \n",
    "    # Store history\n",
    "    history_auto['w'].append(w_auto.item())\n",
    "    history_auto['b'].append(b_auto.item())\n",
    "    history_auto['loss'].append(loss.item())\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iter {iteration:3d}: Loss={loss.item():8.6f}, w={w_auto.item():.4f}, b={b_auto.item():.4f}\")\n",
    "\n",
    "# Transform back to original scale\n",
    "w_auto_original = w_auto * rent.std() / surface.std()\n",
    "b_auto_original = b_auto * rent.std() + rent.mean() - w_auto_original * surface.mean()\n",
    "\n",
    "print(f\"\\nFinal (autograd):     w={w_auto_original.item():.2f}, b={b_auto_original.item():.2f}\")\n",
    "print(f\"Final (manual):       w={w_original.item():.2f}, b={b_original.item():.2f}\")\n",
    "print(f\"True parameters:      w={TRUE_W:.2f}, b={TRUE_B:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. With a Neural Network.\n",
    "\n",
    "Actually, a linear regression is already a very simple instance of a neural network with only one Linear layer.\n",
    "We can use PyTorch neural nets library and a classic training loop to achieve the same result (again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "model.weight.data = torch.tensor([[0.5]]) # Shape (1, 1)\n",
    "model.bias.data = torch.tensor([[0.5]]) # Shape (1, 1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "X_train = surface_std.unsqueeze(1) # Shape: (50, 1)\n",
    "y_train = rent_std.unsqueeze(1) # Shape: (50, 1)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for iteration in range(200):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iter {iteration:3d}: Loss={loss.item():.6f}, w={model.weight.item():.4f}, b={model.bias.item():.4f}\")\n",
    "\n",
    "# After training\n",
    "print(\"\\nFinal model parameters:\")\n",
    "print(f\"Weight (w): {model.weight.item():.4f}\")\n",
    "print(f\"Bias (b):   {model.bias.item():.4f}\")\n",
    "\n",
    "# Transform back to original scale\n",
    "w_nn = model.weight * rent.std() / surface.std()\n",
    "b_nn = model.bias * rent.std() + rent.mean() - w_nn * surface.mean()\n",
    "\n",
    "print(f\"\\nOriginal scale: w={w_nn.item():.2f}, b={b_nn.item():.2f}\")\n",
    "print(f\"True parameters: w={TRUE_W:.2f}, b={TRUE_B:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions\n",
    "\n",
    "Let's use our learned model to predict rent for different apartment sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new apartments\n",
    "test_surfaces = torch.tensor([30.0, 50.0, 70.0, 90.0])\n",
    "predicted_rents = predict(test_surfaces, w_original, b_original)\n",
    "\n",
    "\n",
    "print(predicted_rents)\n",
    "\n",
    "print(\"Rent Predictions for Paris Apartments:\")\n",
    "print(\"=\" * 40)\n",
    "for surf, rent_pred in zip(test_surfaces, predicted_rents):\n",
    "    print(rent_pred)\n",
    "    print(f\"Surface: {surf.item():4.0f} m²  →  Rent: {rent_pred:6.0f} €/month\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nModel: rent = {w_original.item():.1f} × surface + {b_original.item():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "This problem can be solved in 3 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([surface, torch.ones_like(surface)], dim=1)  # Shape: (50, 2)\n",
    "y = rent.unsqueeze(1)\n",
    "solution = torch.linalg.lstsq(X, y)\n",
    "solution.solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025-m2-idl (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
